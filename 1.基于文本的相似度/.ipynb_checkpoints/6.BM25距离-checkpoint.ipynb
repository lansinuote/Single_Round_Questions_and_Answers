{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ff97fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['aa', 'bb', 'cc'],\n",
       "  ['aa', 'bb', 'cc', 'dd'],\n",
       "  ['aa', 'bb', 'cc', 'ee'],\n",
       "  ['aa', 'bb', 'cc', 'ee', 'ee']],\n",
       " ['cc', 'dd', 'ee', 'aa', 'bb'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_docs():\n",
    "    sents = [\n",
    "        'aa bb cc',\n",
    "        'aa bb cc dd',\n",
    "        'aa bb cc ee',\n",
    "        'aa bb cc ee ee',\n",
    "    ]\n",
    "\n",
    "    #分词,过滤停用词\n",
    "    docs = [doc.split() for doc in sents]\n",
    "\n",
    "    #词表\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        words.extend(doc)\n",
    "    words = list(set(words))\n",
    "\n",
    "    return docs, words\n",
    "\n",
    "\n",
    "docs, words = load_docs()\n",
    "\n",
    "docs, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17eda234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 0., 2., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_tf():\n",
    "    #每个文档中的词频\n",
    "    tf = np.zeros([len(docs), len(words)])\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc:\n",
    "            j = words.index(word)\n",
    "            tf[i, j] += 1\n",
    "\n",
    "    return tf\n",
    "\n",
    "\n",
    "tf = get_tf()\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804f868a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 1., 2., 4., 4.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_df():\n",
    "    #每个词在几篇文档中出现\n",
    "    df = np.empty(len(words))\n",
    "    for i, word in enumerate(words):\n",
    "\n",
    "        df[i] = sum([1 for doc in docs if word in doc])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_df()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e088460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10536052, 1.2039728 , 0.69314718, 0.10536052, 0.10536052])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_idf():\n",
    "    #计算idf,衡量了每个词的独特性,越是不独特的词,越接近无穷大,否则接近0\n",
    "    idf = np.empty(len(words))\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        #未出现该词的文档数\n",
    "        fenzi = len(docs) - df[i]\n",
    "\n",
    "        #出现该词的文档数\n",
    "        fenmu = df[i]\n",
    "\n",
    "        #平滑\n",
    "        fenzi += 0.5\n",
    "        fenmu += 0.5\n",
    "\n",
    "        #商的取值是0到正无穷\n",
    "        shang = fenzi / fenmu\n",
    "\n",
    "        #加1,取对数后非负\n",
    "        shang += 1\n",
    "\n",
    "        idf[i] = np.log(shang)\n",
    "\n",
    "    return idf\n",
    "\n",
    "\n",
    "idf = get_idf()\n",
    "\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb2345c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.35614822194194823\n",
      "1 1.5200543512994154\n",
      "2 1.7023759080933698\n",
      "3 2.117234755042299\n"
     ]
    }
   ],
   "source": [
    "def get_score(query, i):\n",
    "\n",
    "    #文档平均长度\n",
    "    mean_len = np.mean([len(doc) for doc in docs])\n",
    "\n",
    "    score = 0.0\n",
    "    for word in query:\n",
    "        #求两份文档词的交集\n",
    "        if word not in docs[i]:\n",
    "            continue\n",
    "\n",
    "        j = words.index(word)\n",
    "\n",
    "        k = 1.5\n",
    "\n",
    "        #词独特性*词频\n",
    "        fenzi = idf[j] * tf[i, j] * (k + 1)\n",
    "\n",
    "        #目标句子的长度和平均长度的商,这个数的值域是0到正无穷\n",
    "        #这意味着,目标句子越长,分母越大,分数也就越低.\n",
    "        #这也是显而易见的,越长的句子内容越分散,权值越低.\n",
    "        deviation = len(docs[i]) / mean_len\n",
    "\n",
    "        #这里的b可以视为一个伸缩系数\n",
    "        #当b=0时,这一项恒定为1\n",
    "        #当b=1时,这一项恒定为deviation\n",
    "        b = 0.75\n",
    "        deviation = 1 - b + b * deviation\n",
    "\n",
    "        fenmu = tf[i, j] + k * deviation\n",
    "\n",
    "        #最终的分数是综合考虑了词的idf,词的数量,目标句子的长度\n",
    "        score += fenzi / fenmu\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "query = 'aa bb cc dd ee ee'.split(' ')\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    print(i, get_score(query, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
