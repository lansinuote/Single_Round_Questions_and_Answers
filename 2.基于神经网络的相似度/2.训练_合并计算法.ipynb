{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 13:56:50.253462: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 13:56:50.477460: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-12 13:56:50.477491: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-12 13:56:51.404875: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 13:56:51.405021: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 13:56:51.405035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3362a434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration embedding-data--sentence-compression-d643585deb6e0073\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/embedding-data___json/embedding-data--sentence-compression-d643585deb6e0073/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(179910,\n",
       " ('The Herzliya wastewater treatment plant has been upgraded to tertiary treatment in accordance with requirements set by the interministerial permits committee for the discharge of waste to the sea and the recommendations of the Marine and Coastal Environment Division of the Ministry of Environmental Protection.',\n",
       "  'Herzliya wastewater treatment plant upgraded',\n",
       "  1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(path='embedding-data/sentence-compression',\n",
    "                               split='train')\n",
    "\n",
    "        self.dataset = dataset.train_test_split(test_size=0.0005)[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s1, s2 = self.dataset[i]['set']\n",
    "        same = 1\n",
    "\n",
    "        #有一半的概率把后半句替换为一句无关的话\n",
    "        if random.random() > 0.5:\n",
    "            s2 = random.choice(self.dataset)['set'][1]\n",
    "            same = 0\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            s1, s2 = s2, s1\n",
    "\n",
    "        return s1, s2, same\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59695a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22488,\n",
       " {'input_ids': tensor([[  101,  2402,  2308,  2007,  7987,  3540, 14494,  2514, 11543,  2055,\n",
       "           7047,  2005,  6550,  1010, 10922,   102,  7643,  2015,  2741,  2055,\n",
       "           2321,  1012,  1017,  2454,  3793,  7696,  2012,  2047,  2095,  1010,\n",
       "           2429,  2000,  4481,  2013,  1996,  2406,  1005,  1055,  2093,  4684,\n",
       "           9224,  1010,  9339,  2011,  7643,  1011,  2653,  3679,  2484, 15775,\n",
       "          11488,  1012,   102],\n",
       "         [  101,  4231, 27137, 13999,  2047,  2489, 27137,  2053, 12816,  3749,\n",
       "            102,  4231, 27137,  3357,  2039,  2971,  2114,  1996,  2502,  3337,\n",
       "           2066,  4419,  2100, 27137,  2011, 10449,  1037,  4435,  2047, 26812,\n",
       "           2489, 27137,  2053, 12816,  2734,  3749,  2007,  1996,  4712,  3642,\n",
       "           4231, 10790,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2091,  7292,  2000,  2330,  2005,  2198,  2745,  8482,   102,\n",
       "           6734,  1010,  2033,  2048,  2047,  6336,  7251,  3794,  6113,  3298,\n",
       "           3417,  8010,  2031,  2042,  2794,  2000,  2148,  5172,  3688,  2759,\n",
       "           2240,  1997, 10457, 22208,  5507,  2063, 13695,  1012,   102,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101, 12645,  3517,  2000,  5271,  2005,  1002,  2184,  4551,   102,\n",
       "           4729,  2152,  1011,  6627,  5016, 12645,  2003,  3517,  2000,  2022,\n",
       "           2853,  2000,  2002, 13668,  6582, 24100,  1999,  1037,  1002,  2184,\n",
       "           4551,  3066,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2103,  2000,  2202,  5056,  1997,  2896,  3037,   102, 14633,\n",
       "           2623,  2651,  2008,  2027,  2031,  9100,  2208,  1997,  6106,  2015,\n",
       "           2005,  1037,  2353,  2161,  1010,  2007,  2186, 17277,  2585,  3841,\n",
       "           3695,  4246,  1998, 16962, 17889,  2012,  1996, 22870,  1012,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2160,  2979,  1036,  1036, 12014,  1005,  1005, 19960,  5555,\n",
       "           3593,  5166,  3021,   102,  1037, 13480,  4362,  2097,  2954,  3674,\n",
       "           9040,  5571,  2058,  2019,  5043,  2029,  9382,  2036,  2920,  2048,\n",
       "          20044, 10028,  2867,  2006,  1996,  2305,  1996, 23848, 13046,  2180,\n",
       "           1996, 11264,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2859,  2003,  2275,  2000,  7505, 15194,  2634,  2004,  1996,\n",
       "           2088,  1005,  1055,  2327,  2751,  7325,  2023,  2095,  1010,  1996,\n",
       "           2088,  2751,  2473, 10173,  1012,   102,  2859,  2000,  7505, 15194,\n",
       "           2634,  2004,  2327,  2751,  7325,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  3290,  2097,  2377,  9562,  1998, 11453,  1999,  2019,  2248,\n",
       "           4715,  2674,  2012,  1022,  7610,  2006, 13114,  1012,  1023,  1010,\n",
       "           2249,  1010,  2012,  1996,  4108,  8514,  1010,  2429,  2000,  1037,\n",
       "           3120,  5220,  2007,  1996,  7776,  1012,   102, 16423,  4363,  2519,\n",
       "           2006,  1996,  2598,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]])},\n",
       " tensor([0, 1, 0, 1, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[:2] for i in data]\n",
    "    same = [i[2] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(sents,\n",
    "                                   truncation=True,\n",
    "                                   padding=True,\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "    same = torch.LongTensor(same)\n",
    "\n",
    "    return data, same\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (data, same) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "len(loader), data, same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6217a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948.3778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #加载预训练模型\n",
    "        self.pretrained = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        #不训练,不需要计算梯度\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        with torch.no_grad():\n",
    "            pred = self.pretrained(**data)\n",
    "\n",
    "        #[b, 768]\n",
    "        pred = pred.last_hidden_state[:, 0]\n",
    "\n",
    "        return self.fc(pred)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "#统计参数量\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)\n",
    "\n",
    "model(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905b80b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5400965213775635 1.0\n",
      "50 0.024473240599036217 1.0\n",
      "100 0.04173409566283226 1.0\n",
      "150 0.012052012607455254 1.0\n",
      "200 0.0048147812485694885 1.0\n",
      "250 0.001603083685040474 1.0\n",
      "300 0.03160906955599785 1.0\n",
      "350 0.0007619516691192985 1.0\n",
      "400 0.007851014845073223 1.0\n",
      "450 0.012219463475048542 1.0\n",
      "500 0.014638349413871765 1.0\n",
      "550 0.053889673203229904 1.0\n",
      "600 0.0024577314034104347 1.0\n",
      "650 0.0006059815059415996 1.0\n",
      "700 0.00053600431419909 1.0\n",
      "750 0.0004944446263834834 1.0\n",
      "800 0.001773191150277853 1.0\n",
      "850 0.0018959895242005587 1.0\n",
      "900 0.001421680673956871 1.0\n",
      "950 0.00024391320766881108 1.0\n",
      "1000 0.002352627459913492 1.0\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "    global model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for i, (data, same) in enumerate(loader):\n",
    "        same = same.to(device)\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "\n",
    "        pred = model(data)\n",
    "\n",
    "        loss = criterion(pred, same)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            pred = pred.argmax(dim=1)\n",
    "            accuracy = (pred == same).sum().item() / len(same)\n",
    "            print(i, loss.item(), accuracy)\n",
    "\n",
    "        if i == 1000:\n",
    "            break\n",
    "\n",
    "    torch.save(model.cpu(), 'models/2.训练_合并计算法.model')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275dd1b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration embedding-data--sentence-compression-d643585deb6e0073\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/embedding-data___json/embedding-data--sentence-compression-d643585deb6e0073/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=16,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=False)\n",
    "\n",
    "    for i, (data, label) in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "\n",
    "        correct += (out == label).sum().item()\n",
    "        total += len(label)\n",
    "\n",
    "        print(i)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "model = torch.load('models/2.训练_合并计算法.model')\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
