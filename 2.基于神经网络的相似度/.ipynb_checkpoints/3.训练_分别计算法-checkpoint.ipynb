{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:54:07.788358: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 14:54:07.937105: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-12 14:54:07.937124: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-12 14:54:08.708706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 14:54:08.708827: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 14:54:08.708861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', vocab_size=250002, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "checkpoint = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3362a434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration embedding-data--sentence-compression-d643585deb6e0073\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/embedding-data___json/embedding-data--sentence-compression-d643585deb6e0073/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(179910,\n",
       " (\"Chris Rock convinced someone is planning a ``sting operation''\",\n",
       "  \"Chris Rock is convinced someone is planning a ``sting operation'' every time a sexy girl talks to him.\",\n",
       "  1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(path='embedding-data/sentence-compression',\n",
    "                               split='train')\n",
    "\n",
    "        self.dataset = dataset.train_test_split(test_size=0.0005)[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s1, s2 = self.dataset[i]['set']\n",
    "        same = 1\n",
    "\n",
    "        #有一半的概率把后半句替换为一句无关的话\n",
    "        if random.random() > 0.5:\n",
    "            s2 = random.choice(self.dataset)['set'][1]\n",
    "            same = 0\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            s1, s2 = s2, s1\n",
    "\n",
    "        return s1, s2, same\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59695a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22488,\n",
       " {'input_ids': tensor([[     0,   5959, 135179,     70,  92702,  93905, 148639,   7432,   8394,\n",
       "           15592,     36,  27751,   3129, 112141,     31,  20051,     98,  17262,\n",
       "             453,     23,     70,  26349,  11698, 110680,    903,   6602,     83,\n",
       "            4420,     47,   6863,  10332,   8299,  16037,      5,      2,      1,\n",
       "               1,      1],\n",
       "         [     0,  18226,  14202,  43542,     47,   4488,     98,   4620,    214,\n",
       "          152132,      7,      2,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1],\n",
       "         [     0,  68804,      7,  10336,    297,    100,     10, 189173,    538,\n",
       "           92319,  67229,      2,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1],\n",
       "         [     0,    581,  51521,  26349,    111,   2501,    748,  31251,  53201,\n",
       "               6, 177476,   9842,      7,     99,    190,   3592,  18925,     23,\n",
       "              70, 105719,  12684,    377,   7568,     23, 139040,   5227,      4,\n",
       "           43980,    382,    927,   2907,      5,    541,      5,      2,      1,\n",
       "               1,      1],\n",
       "         [     0,  26520,    133,     33,  33365,  14465,     20,  26520,    133,\n",
       "              33,  33365,  14465,  16195,    933,  23417,     99,   7270, 133236,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1],\n",
       "         [     0,  30839,     51, 109308,     53,    674, 241957,    538,   9588,\n",
       "              90,   5117,   1733,     23,   6626,   5369,      2,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1],\n",
       "         [     0,    581,    166, 181960,    223,  75186,    450,   1870,   3674,\n",
       "              10,  13036,   1940,      7,  58994,    136,   3884,   6626, 131830,\n",
       "              53, 115215,     98, 110342,  64227,  42141,  69347,   1810,     47,\n",
       "             186,     10,   7401, 204611,      7,      4,  51521,      7,   2804,\n",
       "               5,      2],\n",
       "         [     0, 109383,     53,  38026,      7,    332, 149357,     71,     47,\n",
       "             209,   5369,     23, 101085,     98,  48683,      4,  17863, 124666,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " {'input_ids': tensor([[     0,  75198,     56,    149, 122009,    390,  35206,  50782, 156256,\n",
       "          155358,  11588,      2,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,  18226,  14202,    621,  43542,     47,   4488,    678,  27759,\n",
       "              98,   4620,    214, 152132,      7,      4,  13416,     70,  70615,\n",
       "             111,   5919,     18,    208,    429,  18226,  14202,      5,      2,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,    581,      6,  99721,  93905,  79880, 180327,    390, 113154,\n",
       "           79616,     62,    246,  56888, 182747,   4034,      7,    479,  22751,\n",
       "               7,     98,   7071,   1381,     99,     70,   8129, 146613,      5,\n",
       "               2,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,   2501,    748,  31251,  53201,      6, 177476,   9842,      7,\n",
       "              99,    190,  18925,      2,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,    893, 142297,    332,   2750, 113771,    538,  54324,     70,\n",
       "           17863,  11814,     47,  67153,     10, 149493,  35206,  93324,   1556,\n",
       "            2809,  34784,    297,    136,  61689,   5281,    111,  85325,    538,\n",
       "          143092,  17863,      7,     47,    158,  21605,    933,      5,      2],\n",
       "         [     0, 129335,  69496,     14,  25469,     90,   3542, 152388, 103616,\n",
       "              23,  12921, 162708,  99255,    111,     70,   1257,    277,    214,\n",
       "           84773,  81843,      4,  53190,  39608,   1833, 103310,   8180,      7,\n",
       "               5,      2,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,   1832, 181960,    223,  75186,     99,   1940,      7,  58994,\n",
       "           15504,      7,   1810,     47,    186,   7401, 204611,      7,      2,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1],\n",
       "         [     0,    992,  69723,     19,  14012,    111,  51876,      7,    765,\n",
       "            2809,  11476,     71,     99,     10, 106294, 135440, 172310,    214,\n",
       "           23179,     23,     70,   7082,  11341,    111, 118623,     98,  97001,\n",
       "               4,   2450, 113771,      5,      2,      1,      1,      1,      1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])},\n",
       " tensor([0, 1, 0, 1, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    s1 = [i[0] for i in data]\n",
    "    s2 = [i[1] for i in data]\n",
    "    same = [i[2] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data1 = token.batch_encode_plus(s1,\n",
    "                                    truncation=True,\n",
    "                                    padding=True,\n",
    "                                    max_length=500,\n",
    "                                    return_tensors='pt')\n",
    "\n",
    "    data2 = token.batch_encode_plus(s2,\n",
    "                                    truncation=True,\n",
    "                                    padding=True,\n",
    "                                    max_length=500,\n",
    "                                    return_tensors='pt')\n",
    "\n",
    "    same = torch.LongTensor(same)\n",
    "\n",
    "    return data1, data2, same\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (data1, data2, same) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "len(loader), data1, data2, same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6217a7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #加载预训练模型\n",
    "        self.pretrained = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "        #不训练,不需要计算梯度\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(768, 2),\n",
    "        )\n",
    "\n",
    "    def get_feature(self, data):\n",
    "        with torch.no_grad():\n",
    "            #[b, L, 384]\n",
    "            feature = self.pretrained(**data)['last_hidden_state']\n",
    "\n",
    "        #[b, L]\n",
    "        attention_mask = data['attention_mask']\n",
    "\n",
    "        #pad位置的feature是0\n",
    "        #[b, L, 384] * [b, L, 1] -> [b, L, 384]\n",
    "        feature *= attention_mask.unsqueeze(dim=2)\n",
    "\n",
    "        #所有词的feature求和\n",
    "        #[b, L, 384] -> [b, 384]\n",
    "        feature = feature.sum(dim=1)\n",
    "\n",
    "        #求和后的feature除以句子的长度\n",
    "        #[b, L] -> [b, 1]\n",
    "        attention_mask = attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        #[b, 384] / [b, 1] -> [b, 384]\n",
    "        feature /= attention_mask.clamp(min=1e-8)\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        feature1 = self.get_feature(data1)\n",
    "        feature2 = self.get_feature(data2)\n",
    "\n",
    "        feature = torch.cat([feature1, feature2], dim=1)\n",
    "\n",
    "        return self.fc(feature)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(data1, data2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905b80b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6659839153289795 0.875\n",
      "50 0.6840968132019043 0.375\n",
      "100 0.6825852394104004 0.5\n",
      "150 0.5992887616157532 0.75\n",
      "200 0.6148892641067505 0.875\n",
      "250 0.6363856196403503 0.75\n",
      "300 0.49421191215515137 0.875\n",
      "350 0.48576438426971436 0.875\n",
      "400 0.4708303213119507 0.75\n",
      "450 0.45533663034439087 0.75\n",
      "500 0.2757263481616974 0.875\n",
      "550 0.21047598123550415 1.0\n",
      "600 0.22561021149158478 0.875\n",
      "650 0.16972705721855164 1.0\n",
      "700 0.2094234973192215 0.875\n",
      "750 0.11808156967163086 1.0\n",
      "800 0.24072951078414917 0.875\n",
      "850 0.23320215940475464 0.875\n",
      "900 0.048693425953388214 1.0\n",
      "950 0.0808393731713295 1.0\n",
      "1000 0.06140816956758499 1.0\n",
      "1050 0.025953814387321472 1.0\n",
      "1100 0.05567248910665512 1.0\n",
      "1150 0.1256197690963745 1.0\n",
      "1200 0.13684037327766418 1.0\n",
      "1250 0.02317812480032444 1.0\n",
      "1300 0.18239809572696686 0.875\n",
      "1350 0.12977005541324615 0.875\n",
      "1400 0.011395378038287163 1.0\n",
      "1450 0.09584089368581772 1.0\n",
      "1500 0.4376535415649414 0.875\n",
      "1550 0.1762787252664566 0.875\n",
      "1600 0.15225888788700104 0.875\n",
      "1650 0.1379726231098175 0.875\n",
      "1700 0.349697470664978 0.75\n",
      "1750 0.01531105674803257 1.0\n",
      "1800 0.013113302178680897 1.0\n",
      "1850 0.014158674515783787 1.0\n",
      "1900 0.032894834876060486 1.0\n",
      "1950 0.026065263897180557 1.0\n",
      "2000 0.01210873294621706 1.0\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "def train():\n",
    "    global model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for i, (data1, data2, same) in enumerate(loader):\n",
    "        same = same.to(device)\n",
    "        for k in data1.keys():\n",
    "            data1[k] = data1[k].to(device)\n",
    "            data2[k] = data2[k].to(device)\n",
    "        pred = model(data1, data2)\n",
    "\n",
    "        loss = criterion(pred, same)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            pred = pred.argmax(dim=1)\n",
    "            accuracy = (pred == same).sum().item() / len(same)\n",
    "            print(i, loss.item(), accuracy)\n",
    "\n",
    "        if i == 2000:\n",
    "            break\n",
    "\n",
    "    torch.save(model.cpu(), 'models/3.训练_分别计算法.model')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275dd1b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration embedding-data--sentence-compression-d643585deb6e0073\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/embedding-data___json/embedding-data--sentence-compression-d643585deb6e0073/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0.9888888888888889\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=16,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=False)\n",
    "\n",
    "    for i, (data1, data2, label) in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            pred = model(data1, data2)\n",
    "\n",
    "        pred = pred.argmax(dim=1)\n",
    "\n",
    "        correct += (pred == label).sum().item()\n",
    "        total += len(label)\n",
    "\n",
    "        print(i)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "model = torch.load('models/3.训练_分别计算法.model')\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
