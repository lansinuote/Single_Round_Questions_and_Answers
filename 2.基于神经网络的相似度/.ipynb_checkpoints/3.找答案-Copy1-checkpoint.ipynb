{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54790df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', vocab_size=250002, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "checkpoint = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ff297c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff02fe10926e4cee8b2a9b95e7bca9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-6a0e212c0c0fa0d4.arrow and /root/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-e52c39cad4c58186.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15862,\n",
       " {'question': 'is profit and loss statement and income statement same',\n",
       "  'passage': \"Income statement -- An income statement or profit and loss account (also referred to as a profit and loss statement (P&L), statement of profit or loss, revenue statement, statement of financial performance, earnings statement, operating statement, or statement of operations) is one of the financial statements of a company and shows the company's revenues and expenses during a particular period. It indicates how the revenues (money received from the sale of products and services before expenses are taken out, also known as the ``top line'') are transformed into the net income (the result after all revenues and expenses have been accounted for, also known as ``net profit'' or the ``bottom line''). The purpose of the income statement is to show managers and investors whether the company made or lost money during the period being reported.\",\n",
       "  'idx': 6874,\n",
       "  'label': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(path='super_glue', name='boolq')\n",
    "\n",
    "        #重新切分数据集\n",
    "        dataset = concatenate_datasets([dataset[i] for i in dataset.keys()])\n",
    "        self.dataset = dataset.train_test_split(test_size=0.005, seed=0)[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i]\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a98f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1983,\n",
       " {'input_ids': tensor([[     0,     83,  18348,    136,  86669,  63805,    136,  91763,  63805,\n",
       "            5701,      2,      1,      1,      1,      1],\n",
       "         [     0,     83,     70,  56877,    289, 106820,     70,   5701,    237,\n",
       "              70, 106820,    111,  16095,   1760,      2],\n",
       "         [     0,     83,   2565,   4299,     70,   5701,  13580,    237,      6,\n",
       "          138410,   2565,      2,      1,      1,      1],\n",
       "         [     0,   1556,     10,  46667,  17669,   2809, 144888,    297,     23,\n",
       "              70,    653,    402,      2,      1,      1],\n",
       "         [     0,     83,   2685,     10,  25550,  29398,     98,   2054,   2069,\n",
       "           90695,      2,      1,      1,      1,      1],\n",
       "         [     0,  14602,     70, 192182,  15889,  12936,   7701,    765,   2499,\n",
       "           34153,      2,      1,      1,      1,      1],\n",
       "         [     0,     83,     70,   4842,  94897,     10,    263,     71,    111,\n",
       "             479,     53,      2,      1,      1,      1],\n",
       "         [     0,     83,   8167,   9248,  17901,     98,     70,  29256,    111,\n",
       "            1286,   6897,      2,      1,      1,      1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    question = [i['question'] for i in data]\n",
    "\n",
    "    #编码\n",
    "    return token.batch_encode_plus(question,\n",
    "                                   truncation=True,\n",
    "                                   padding=True,\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt')\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=False,\n",
    "                                     drop_last=False)\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    break\n",
    "\n",
    "len(loader), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae8ab7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (pretrained): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(250037, 384, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 384)\n",
       "      (token_type_embeddings): Embedding(2, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "#定义模型\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #加载预训练模型\n",
    "        self.pretrained = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "        #不训练,不需要计算梯度\n",
    "        for param in self.pretrained.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 768),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(768, 2),\n",
    "        )\n",
    "\n",
    "    def get_feature(self, data):\n",
    "        with torch.no_grad():\n",
    "            #[b, L, 384]\n",
    "            feature = self.pretrained(**data)['last_hidden_state']\n",
    "\n",
    "        #[b, L]\n",
    "        attention_mask = data['attention_mask']\n",
    "\n",
    "        #pad位置的feature是0\n",
    "        #[b, L, 384] * [b, L, 1] -> [b, L, 384]\n",
    "        feature *= attention_mask.unsqueeze(dim=2)\n",
    "\n",
    "        #所有词的feature求和\n",
    "        #[b, L, 384] -> [b, 384]\n",
    "        feature = feature.sum(dim=1)\n",
    "\n",
    "        #求和后的feature除以句子的长度\n",
    "        #[b, L] -> [b, 1]\n",
    "        attention_mask = attention_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        #[b, 384] / [b, 1] -> [b, 384]\n",
    "        feature /= attention_mask.clamp(min=1e-8)\n",
    "\n",
    "        return feature\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        feature1 = self.get_feature(data1)\n",
    "        feature2 = self.get_feature(data2)\n",
    "\n",
    "        feature = torch.cat([feature1, feature2], dim=1)\n",
    "\n",
    "        return self.fc(feature)\n",
    "\n",
    "\n",
    "model = torch.load('models/2.求相似度_分别计算法.model')\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8236c73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n"
     ]
    }
   ],
   "source": [
    "#构建知识矩阵\n",
    "def build_features():\n",
    "    global model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    features = []\n",
    "    for i, data in enumerate(loader):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "\n",
    "        features.append(model.get_feature(data))\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(i)\n",
    "\n",
    "    model.cpu()\n",
    "\n",
    "    features = torch.cat(features)\n",
    "\n",
    "    torch.save(features.cpu(), 'models/features.pt')\n",
    "\n",
    "\n",
    "build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ca0ae6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/root/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc425eed171d4bb29c2736e7dd10c967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-6a0e212c0c0fa0d4.arrow and /root/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-e52c39cad4c58186.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.9841532111167908\n",
      "do all bacteria have peptidoglycan in their cell walls\n",
      "can there be two foreign keys in a table\n",
      "1\n",
      "0.9935773015022278\n",
      "curium-242 was synthesized by bombarding an isotope with alpha particles\n",
      "can a person from california buy a gun in arizona\n",
      "2\n",
      "0.9999700784683228\n",
      "can you block the plate in college baseball\n",
      "do you have to get out of the way of a pitch\n",
      "3\n",
      "0.9998216032981873\n",
      "does the color of a flame indicate its temperature\n",
      "can you have a turbo and a supercharger at the same time\n",
      "4\n",
      "0.9998830556869507\n",
      "is it illegal to wear clothes and return them\n",
      "is hand baggage the same as carry on\n",
      "5\n",
      "0.9999994039535522\n",
      "is stock price the same as share price\n",
      "can you do a like kind exchange on stock\n",
      "6\n",
      "0.9989058971405029\n",
      "did candace cameron win dancing with the stars\n",
      "did a magician ever win americas got talent\n",
      "7\n",
      "0.9999935626983643\n",
      "is the caribbean sea part of the atlantic\n",
      "is the caribbean part of the atlantic ocean\n",
      "8\n",
      "0.9999872446060181\n",
      "were the mamma mia songs written for the movie\n",
      "was pretty in pink written for the movie\n",
      "9\n",
      "0.9998921155929565\n",
      "is east st louis part of st louis\n",
      "is st louis city in st louis county\n",
      "10\n",
      "0.9703049063682556\n",
      "was the laramide orogeny related to subduction of an oceanic plateau\n",
      "can a ship go from the great lakes to the ocean\n",
      "11\n",
      "0.9999117851257324\n",
      "is the movie mine based on a true story\n",
      "was the movie i can only imagine based on a true story\n",
      "12\n",
      "0.9999017715454102\n",
      "does rory go back to yale in season 6\n",
      "is daredevil coming back for a third season\n",
      "13\n",
      "0.9998527765274048\n",
      "do the contestants win money on to tell the truth\n",
      "does only the winner get money on tipping point\n",
      "14\n",
      "0.9993558526039124\n",
      "is suny buffalo the same as university of buffalo\n",
      "are there any wild bison in north america\n",
      "15\n",
      "0.9999996423721313\n",
      "do the jets and giants share a stadium\n",
      "do the giants and jets share metlife stadium\n",
      "16\n",
      "0.948456346988678\n",
      "can a person score high in both neuroticism and extraversion\n",
      "is lucky for life a multi state game\n",
      "17\n",
      "0.9998774528503418\n",
      "is the cerebral cortex part of the limbic system\n",
      "is the limbic system part of the cerebral cortex\n",
      "18\n",
      "0.9999991655349731\n",
      "is it possible for twins to have different fathers\n",
      "can you have twins by two different fathers\n",
      "19\n",
      "0.9997647404670715\n",
      "does birth certificate count as a form of id\n",
      "is a birth certificate a valid form of id\n",
      "20\n",
      "0.999990701675415\n",
      "are they still making the show once upon a time\n",
      "is once upon a time tv show over\n",
      "21\n",
      "0.9991945624351501\n",
      "is polka king based on a true story\n",
      "is never let me go a true story\n",
      "22\n",
      "0.9996464252471924\n",
      "is there any lake found near the meanders of the river\n",
      "is there a dam on the missouri river\n",
      "23\n",
      "0.9999909400939941\n",
      "do you have to leave a return address\n",
      "will a letter be delivered without a return address\n",
      "24\n",
      "0.9999479055404663\n",
      "has anyone ever found a four leaf clover\n",
      "can you actually find a four leaf clover\n",
      "25\n",
      "0.999992847442627\n",
      "is the uk part of the schengen zone\n",
      "are norway sweden and finland part of europe\n",
      "26\n",
      "0.9860806465148926\n",
      "is pamantasan ng lungsod ng maynila a good school\n",
      "do you have to be lds to attend byu\n",
      "27\n",
      "0.9999866485595703\n",
      "can a 14 year old shoot a gun at a shooting range\n",
      "do you need your own gun to go to a shooting range\n",
      "28\n",
      "0.9999946355819702\n",
      "did the usa soccer team qualify for the world cup\n",
      "has america ever made it to the world cup\n",
      "29\n",
      "0.9940541386604309\n",
      "is the movie split part of a trilogy\n",
      "will there be a sequel to the movie predators\n",
      "30\n",
      "0.9997417330741882\n",
      "is there a rain forest in the united states\n",
      "are there any rainforests in the united states\n",
      "31\n",
      "0.9999966621398926\n",
      "is beauty and the beast still on broadway\n",
      "is there a sequel to love finds a home\n",
      "32\n",
      "0.9999833106994629\n",
      "do you have to join a union in a right to work state\n",
      "are there still unions in right to work states\n",
      "33\n",
      "0.9999988079071045\n",
      "does everyone on are you the one get a million dollars\n",
      "has anyone ever won a million on who wants to be a millionaire\n",
      "34\n",
      "0.999980092048645\n",
      "is there a season 5 of the origionals\n",
      "can you get tri tip on the east coast\n",
      "35\n",
      "0.9999527931213379\n",
      "is ofac a division of the united nations\n",
      "is unicef a part of the united nations\n",
      "36\n",
      "0.9999203681945801\n",
      "is an older brother considered a legal guardian\n",
      "is an administrative law judge a real judge\n",
      "37\n",
      "0.9975715279579163\n",
      "do the outlaws and hells angels get along\n",
      "does rainbow six siege have a story mode\n",
      "38\n",
      "0.9999984502792358\n",
      "is the jaws ride still at universal studios orlando\n",
      "is there a jaws ride at universal studios hollywood\n",
      "39\n",
      "0.9964476823806763\n",
      "is a pension fund a collective investment scheme\n",
      "is a money service business a financial institution\n",
      "40\n",
      "0.9999679327011108\n",
      "is there a toll on the battery tunnel\n",
      "are there any tolls on i-75\n",
      "41\n",
      "0.9999148845672607\n",
      "is there a salary cap in the nhl\n",
      "can you go over the salary cap in the nhl\n",
      "42\n",
      "0.9994053840637207\n",
      "do female and male blue jays look the same\n",
      "is it rare to see a blue jay\n",
      "43\n",
      "0.9999420642852783\n",
      "did colombia make it to the round of 16\n",
      "did mexico made it to the world cup\n",
      "44\n",
      "0.9999368190765381\n",
      "nilgiri himalyan and arabian are types of tahr\n",
      "nilgiri himalayan and arabian are types of animals\n",
      "45\n",
      "0.9999840259552002\n",
      "is there going to be a season 2 for iron fist\n",
      "is there a second season of iron fist\n",
      "46\n",
      "0.9994871616363525\n",
      "are high rise and high waisted jeans the same\n",
      "can you turn right on red in la\n",
      "47\n",
      "0.9999877214431763\n",
      "has anyone won a grammy and an oscar\n",
      "has anyone ever won an oscar and a grammy\n",
      "48\n",
      "0.9988387227058411\n",
      "does a notary have to be a lawyer\n",
      "is it compulsory to perform jury duty if called upon\n",
      "49\n",
      "0.9999957084655762\n",
      "is chili sauce the same as cocktail sauce\n",
      "is tabasco sauce and hot sauce the same\n",
      "50\n",
      "0.999894380569458\n",
      "do i use plumbers tape on compression fittings\n",
      "is there a mechanical advantage for a fixed pulley\n",
      "51\n",
      "0.9999960660934448\n",
      "can you make coffee from a kentucky coffee tree\n",
      "does a frappe have coffee in it starbucks\n",
      "52\n",
      "0.9996885061264038\n",
      "can i use cast iron skillet on induction cooktop\n",
      "can you use a cast iron skillet on an induction stove\n",
      "53\n",
      "0.9999822378158569\n",
      "can you drink on sunday in north carolina\n",
      "can you buy liquor on sunday in maine\n",
      "54\n",
      "0.9995296001434326\n",
      "is an employment authorization card a green card\n",
      "is a green card a valid form of id\n",
      "55\n",
      "0.9313147664070129\n",
      "can quantum computers solve np complete problems in polynomial time\n",
      "are the ford ka and fiat 500 the same\n",
      "56\n",
      "0.9999977350234985\n",
      "is filet mignon part of a porterhouse steak\n",
      "is porterhouse steak same as t-bone\n",
      "57\n",
      "0.9973821043968201\n",
      "is the ace the highest card in war\n",
      "can an ace be used in a straight\n",
      "58\n",
      "0.99588543176651\n",
      "is radius of curvature same as centre of curvature\n",
      "is center of gravity the same as centroid\n",
      "59\n",
      "0.9999955892562866\n",
      "does the maloof family own the sacramento kings\n",
      "can you checkmate with two bishops and a king\n",
      "60\n",
      "0.9999966621398926\n",
      "is a 1-888 number toll free\n",
      "is 1-877 a toll free number\n",
      "61\n",
      "0.9998611211776733\n",
      "has any horse won triple crown since secretariat\n",
      "do you get to keep the triple crown trophy\n",
      "62\n",
      "0.9989691972732544\n",
      "will there be a sequal to i am number four\n",
      "can you be left handed and right handed\n",
      "63\n",
      "0.9999957084655762\n",
      "has anyone ever tied for an olympic medal\n",
      "has anyone ever been awarded two medal of honors\n",
      "64\n",
      "0.9999840259552002\n",
      "can a minor be sentenced to the death penalty\n",
      "can someone with a life sentence get parole\n",
      "65\n",
      "0.9999686479568481\n",
      "is the movie the wall a true story\n",
      "is the wall movie based on true story\n",
      "66\n",
      "0.9933306574821472\n",
      "is alternative hypothesis the same as research hypotheses\n",
      "are research and scientific method related to one another\n",
      "67\n",
      "0.9999775886535645\n",
      "has anyone ever hit for the cycle twice in one season\n",
      "has anyone ever hit 2 grand slams in one game\n",
      "68\n",
      "0.9992398023605347\n",
      "is the janitor from scrubs really in the fugitive\n",
      "does the janitor in scrubs have a name\n",
      "69\n",
      "0.9996961355209351\n",
      "was the great chicago fire started by a cow\n",
      "is chicago fire filmed in a real firehouse\n",
      "70\n",
      "1.0\n",
      "is there a bridge from morocco to spain\n",
      "is there a bridge between morocco and spain\n",
      "71\n",
      "0.9537296295166016\n",
      "can you breed a dog with a dingo\n",
      "does a mug have to have a handle\n",
      "72\n",
      "0.9978729486465454\n",
      "is the sarcolemma the same as the endomysium\n",
      "can there be two foreign keys in a table\n",
      "73\n",
      "0.9996623992919922\n",
      "are killer whales apart of the dolphin family\n",
      "is the killer whale part of the dolphin family\n",
      "74\n",
      "0.9368491172790527\n",
      "does doom 3 bfg edition include resurrection of evil\n",
      "is beyond good and evil 2 a prequel\n",
      "75\n",
      "0.9999935626983643\n",
      "is there such a thing as a passport card\n",
      "is a us passport card a valid form of id\n",
      "76\n",
      "0.9918991327285767\n",
      "are antisocial personality disorder and psychopathy the same\n",
      "is it bad to bite the skin off your fingers\n",
      "77\n",
      "0.9987483024597168\n",
      "do thelma and louise die at the end\n",
      "did thelma and louise die in the movie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "0.9999995231628418\n",
      "can you buy beer on sunday in new york\n",
      "can you buy beer in new york state on sunday\n",
      "79\n",
      "0.9999910593032837\n",
      "are the oscars the same as the academy awards\n",
      "are oscars and academy awards the same thing\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=1,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=False,\n",
    "                                              drop_last=False)\n",
    "\n",
    "    for i, data in enumerate(loader_test):\n",
    "        feature = model.get_feature(data)\n",
    "        \n",
    "        feature = feature.repeat(features.shape[0], 1)\n",
    "        \n",
    "        feature = torch.cat([features, feature], dim=1)\n",
    "\n",
    "        score = model.fc(feature).softmax(dim=1)[:,1]\n",
    "\n",
    "        argmax = score.argmax().item()\n",
    "\n",
    "        if score[argmax].item() > 0.99:\n",
    "            print(i)\n",
    "            print(score[argmax].item())\n",
    "            print(token.decode(data['input_ids'][0], skip_special_tokens=True))\n",
    "            print(dataset[argmax]['question'])\n",
    "\n",
    "\n",
    "features = torch.load('models/features.pt')\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
